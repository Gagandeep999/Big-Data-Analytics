{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3612jvsc74a57bd0393438f6c0bca4a2b0b9b28a96f5952f2521bd7399947aa24a0a5b669f83e1d6",
   "display_name": "Python 3.6.12 64-bit ('bigdata': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import CountVectorizer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/Users/Gagandeep/Desktop/Concordia/12 Winter 2021/SOEN 471/NoToW/data/raw'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "head, tail = os.path.split(os.getcwd())\n",
    "data_dir = os.path.join(head, 'data')\n",
    "data_raw_dir = os.path.join(data_dir, 'raw')\n",
    "DATA_INTERIM_DIR = os.path.join(data_dir, 'interim')\n",
    "DATA_PROCESSED_DIR = os.path.join(data_dir, 'processed')\n",
    "data_raw_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(DATE_ORIGINE='2019-01-10', LONGITUDE_ORIGINE=-73.60370215594651, LATITUDE_ORIGINE=45.5590991399067, Distance_km=0.0484305337462778, MOTIF_REMORQUAGE=\"Constat d'infraction\", Date_Time='2019-01-10', Year=2019.0, Month=1.0, Day=10.0, Mean_Temp=-7.9, Total_Rain=0.0, Total_Precip=0.2, Total_Snow=0.0, Spd_of_Max_Gust=48.0),\n",
       " Row(DATE_ORIGINE='2019-01-10', LONGITUDE_ORIGINE=-73.599733540326, LATITUDE_ORIGINE=45.453884709812606, Distance_km=0.07233637828159714, MOTIF_REMORQUAGE=\"Constat d'infraction\", Date_Time='2019-01-10', Year=2019.0, Month=1.0, Day=10.0, Mean_Temp=-7.9, Total_Rain=0.0, Total_Precip=0.2, Total_Snow=0.0, Spd_of_Max_Gust=48.0)]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "clean_data = spark.read.parquet(os.path.join(DATA_PROCESSED_DIR, 'cleaned.data'))\n",
    "clean_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.isNull().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['LONGITUDE_ORIGINE', 'LATITUDE_ORIGINE', 'Month', 'Day', 'Mean_Temp', 'Total_Rain', 'Total_Precip', 'Total_Snow']\n",
      "+------------------+------------------+-----+----+---------+----------+------------+----------+\n",
      "| LONGITUDE_ORIGINE|  LATITUDE_ORIGINE|Month| Day|Mean_Temp|Total_Rain|Total_Precip|Total_Snow|\n",
      "+------------------+------------------+-----+----+---------+----------+------------+----------+\n",
      "|-73.60370215594651|  45.5590991399067|  1.0|10.0|     -7.9|       0.0|         0.2|       0.0|\n",
      "|  -73.599733540326|45.453884709812606|  1.0|10.0|     -7.9|       0.0|         0.2|       0.0|\n",
      "| -73.5575876924943|45.531104742785395|  1.0|10.0|     -7.9|       0.0|         0.2|       0.0|\n",
      "| -73.5599771223213|   45.594984152964|  1.0|10.0|     -7.9|       0.0|         0.2|       0.0|\n",
      "|-73.62714397061728|  45.4186077928051|  1.0|10.0|     -7.9|       0.0|         0.2|       0.0|\n",
      "+------------------+------------------+-----+----+---------+----------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = clean_data.drop('DATE_ORIGINE','Distance_km','MOTIF_REMORQUAGE','Date_Time','Year', 'Spd_of_Max_Gust')\n",
    "print(features.columns)\n",
    "features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in features.schema.names:\n",
    "    if features.filter(features[col_name].isNull()).count() > 0:\n",
    "        print(col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+--------------------+\n|            features|         Distance_km|\n+--------------------+--------------------+\n|[-73.603702155946...|  0.0484305337462778|\n|[-73.599733540326...| 0.07233637828159714|\n|[-73.557587692494...|  0.3967286084324585|\n|[-73.559977122321...|  0.2499872949122831|\n|[-73.627143970617...|0.047350695587300755|\n+--------------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols = features.columns, outputCol = 'features')\n",
    "training_df = vectorAssembler.transform(clean_data)\n",
    "training_df = training_df.select(['features', 'Distance_km'])\n",
    "training_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(training_df))\n",
    "train,test = training_df.randomSplit([0.75, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol = 'features', labelCol='Distance_km')\n",
    "lr_model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Coefficients: [-0.30150542959901183,0.1328908652697921,-0.0019920841747191106,0.00020015526289856626,-0.00060211949497434,0.0,0.00029682119789491704,0.0]\n\nIntercept: -27.957594402754818\n"
     ]
    }
   ],
   "source": [
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"\\nIntercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE: 0.362298\n\nr2: 0.001919\n"
     ]
    }
   ],
   "source": [
    "trainSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainSummary.rootMeanSquaredError)\n",
    "print(\"\\nr2: %f\" % trainSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Coefficients: [-0.30150542959901183,0.1328908652697921,-0.0019920841747191106,0.00020015526289856626,-0.00060211949497434,0.0,0.00029682119789491704,0.0]\n\nIntercept: -27.957594402754818\nRMSE: 0.362298\n\nr2: 0.001919\n"
     ]
    }
   ],
   "source": [
    "lr2 = LinearRegression(featuresCol = 'features', labelCol='Distance_km',maxIter=1000, regParam=0.12, elasticNetParam=0.2)\n",
    "lr_model2 = lr.fit(train)\n",
    "print(\"Coefficients: \" + str(lr_model2.coefficients))\n",
    "print(\"\\nIntercept: \" + str(lr_model2.intercept))\n",
    "trainSummary = lr_model2.summary\n",
    "print(\"RMSE: %f\" % trainSummary.rootMeanSquaredError)\n",
    "print(\"\\nr2: %f\" % trainSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+--------------------+------------------+--------------------+\n|         prediction|         Distance_km|          Accuracy|            features|\n+-------------------+--------------------+------------------+--------------------+\n|0.31948522894599307| 0.07774081929023328| 310.9620040833948|(8,[0,1,2,3],[-73...|\n|  0.319004030864086| 0.32676351753179644| 2.374649020282802|(8,[0,1,2,3],[-73...|\n|  0.301166570206032| 0.04211591656085911| 615.0896734512112|(8,[0,1,2,3],[-73...|\n|  0.298204122527828| 0.06608208888522123|351.26316004595793|(8,[0,1,2,3],[-73...|\n|  0.316183658437172|0.005464875812922718| 5685.742791986174|(8,[0,1,2,3],[-73...|\n| 0.2975068433572474|                 0.0|              null|(8,[0,1,2,3],[-73...|\n| 0.2975068433572474|  0.7323478551560163| 59.37629348366593|(8,[0,1,2,3],[-73...|\n|0.29709414832123215| 0.20156398653590432|    47.39445941069|(8,[0,1,2,3],[-73...|\n|0.29709414832123215|  0.6283447658479964|52.717971968735625|(8,[0,1,2,3],[-73...|\n| 0.2851806922774678|0.007259368694066...|3828.4503142893827|(8,[0,1,2,3],[-73...|\n| 0.3135095531319365| 0.06539027099498773|379.44372819003536|(8,[0,1,2,3],[-73...|\n| 0.3138907868909264| 0.10239238093755287|206.55678090185475|(8,[0,1,2,3],[-73...|\n|0.31543524045093463|0.028310040021834664|1014.2168651391842|(8,[0,1,2,3],[-73...|\n| 0.3149669240335484|0.028946685382207488| 988.0932302775764|(8,[0,1,2,3],[-73...|\n|0.29480490091645706|  0.7484708618665552| 60.61237438405214|(8,[0,1,2,3],[-73...|\n| 0.3139678501265344| 0.12421815001062385| 152.7552133884477|(8,[0,1,2,3],[-73...|\n| 0.3145029050172141| 0.48947971454791916| 35.74750992333006|(8,[0,1,2,3],[-73...|\n|0.30940658811200805| 0.08148796121735252|279.69607226609713|(8,[0,1,2,3],[-73...|\n| 0.3109938086269288|0.001481408616591...| 20893.11460348705|(8,[0,1,2,3],[-73...|\n|0.31158384849910803|   0.396183013410141|21.353556827902988|(8,[0,1,2,3],[-73...|\n+-------------------+--------------------+------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from  pyspark.sql.functions import abs\n",
    "predictions = lr_model.transform(test)\n",
    "x =((predictions['Distance_km']-predictions['prediction'])/predictions['Distance_km'])*100\n",
    "predictions = predictions.withColumn('Accuracy',abs(x))\n",
    "predictions.select(\"prediction\",\"Distance_km\",\"Accuracy\",\"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'lr_model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-47ebe5b16d4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lr_model' is not defined"
     ]
    }
   ],
   "source": [
    "results = lr_model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}