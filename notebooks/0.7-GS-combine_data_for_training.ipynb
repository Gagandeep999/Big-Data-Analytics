{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3612jvsc74a57bd0393438f6c0bca4a2b0b9b28a96f5952f2521bd7399947aa24a0a5b669f83e1d6",
   "display_name": "Python 3.6.12 64-bit ('bigdata': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head, tail = os.path.split(os.getcwd())\n",
    "data_dir = os.path.join(head, 'data')\n",
    "data_raw_dir = os.path.join(data_dir, 'raw')\n",
    "DATA_INTERIM_DIR = os.path.join(data_dir, 'interim')\n",
    "data_raw_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = init_spark()\n",
    "\n",
    "data_combined_2015 = spark.read.parquet(os.path.join(DATA_INTERIM_DIR, 'weather_2015.data'))\n",
    "\n",
    "data_combined_2016 = spark.read.parquet(os.path.join(DATA_INTERIM_DIR, 'weather_2016.data'))\n",
    "# data_combined_2016 = data_combined_2016.na.fill(0)\n",
    "data_combined_2017 = spark.read.parquet(os.path.join(DATA_INTERIM_DIR, 'weather_2017.data'))\n",
    "# data_combined_2017 = data_combined_2017.na.fill(0)\n",
    "data_combined_2018 = spark.read.parquet(os.path.join(DATA_INTERIM_DIR, 'weather_2018.data'))\n",
    "# data_combined_2018 = data_combined_2018.na.fill(0)\n",
    "data_combined_2019 = spark.read.parquet(os.path.join(DATA_INTERIM_DIR, 'weather_2019.data'))\n",
    "# data_combined_2019 = data_combined_2019.na.fill(0)\n",
    "data_combined_2020 = spark.read.parquet(os.path.join(DATA_INTERIM_DIR, 'weather_2020.data'))\n",
    "# data_combined_2020 = data_combined_2020.na.fill(0)\n",
    "l = [data_combined_2015, data_combined_2016, data_combined_2017, data_combined_2018, data_combined_2019, data_combined_2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "null_cols = []\n",
    "for col_name in data_combined_2015.schema.names:\n",
    "    if data_combined_2015.filter(data_combined_2015[col_name].isNull()).count() > 0:\n",
    "        print(col_name)\n",
    "        null_cols.append(col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for null_col_name in null_cols:\n",
    "    data_combined_2015 = data_combined_2015.fillna({null_col_name:'0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each_file in l:\n",
    "    null_cols = []\n",
    "    for col_name in each_file.schema.names:\n",
    "        if each_file.filter(each_file[col_name].isNull()).count() > 0:\n",
    "            null_cols.append(col_name)\n",
    "    \n",
    "    for null_col_name in null_cols:\n",
    "        each_file = each_file.fillna({null_col_name:'0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = l[0].unionByName(l[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, len(l)):\n",
    "    df_final = df_final.unionByName(l[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_combined_2015.count())\n",
    "print(data_combined_2016.count())\n",
    "print(data_combined_2017.count())\n",
    "print(data_combined_2018.count())\n",
    "print(data_combined_2019.count())\n",
    "print(data_combined_2020.count())\n",
    "print(df_final.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}